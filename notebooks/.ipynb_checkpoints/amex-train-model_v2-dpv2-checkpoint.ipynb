{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T06:08:55.379396Z",
     "iopub.status.busy": "2022-08-07T06:08:55.378107Z",
     "iopub.status.idle": "2022-08-07T06:08:55.412431Z",
     "shell.execute_reply": "2022-08-07T06:08:55.411149Z",
     "shell.execute_reply.started": "2022-08-07T06:08:55.379267Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T06:08:55.415365Z",
     "iopub.status.busy": "2022-08-07T06:08:55.414595Z",
     "iopub.status.idle": "2022-08-07T06:08:58.401412Z",
     "shell.execute_reply": "2022-08-07T06:08:58.399724Z",
     "shell.execute_reply.started": "2022-08-07T06:08:55.415313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "from math import sqrt\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "import math as mt\n",
    "from math import *\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "import joblib\n",
    "import random\n",
    "import itertools\n",
    "import scipy as sp\n",
    "\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T06:08:58.462180Z",
     "iopub.status.busy": "2022-08-07T06:08:58.461074Z",
     "iopub.status.idle": "2022-08-07T06:08:58.492634Z",
     "shell.execute_reply": "2022-08-07T06:08:58.491357Z",
     "shell.execute_reply.started": "2022-08-07T06:08:58.462119Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_feats = ['B_30','B_38','D_114','D_116','D_117','D_120','D_126','D_63','D_64','D_66','D_68']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    input_dir = '../src/data/processed/'\n",
    "    dpv = 'v2'\n",
    "    mv = 'v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_public.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_public.parquet')\n",
    "    test['D_86_last'] = np.where(test['D_86_last']==-1, 0, test['D_86_last'])\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 2358), (924621, 2357))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(CFG.seed)\n",
    "df_train, df_test = read_data()\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, X_test, cat_feats):\n",
    "    \n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_feats]\n",
    "\n",
    "#     cat_features = ([f\"{cf}_last\" for cf in cat_feats]+[f\"{cf}_first\" for cf in cat_feats]+\n",
    "#                     [f\"{cf}_median\" for cf in cat_feats])\n",
    "    \n",
    "    add_cats = ([col for col in train.columns.tolist() if f\"_isFirst\" in col]+\n",
    "                [col for col in train.columns.tolist() if f\"_isLast\" in col])\n",
    "    \n",
    "    # Label encode categorical features\n",
    "    for cat_col in cat_features+add_cats:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        X_test[cat_col] = encoder.transform(X_test[cat_col])\n",
    "        \n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        X_test[col + '_round2'] = X_test[col].round(2)\n",
    "    \n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    \n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            X_test[f'{col}_last_mean_diff'] = X_test[f'{col}_last'] - X_test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    \n",
    "    print('numeric cols: ',len(num_cols))\n",
    "#     for col in tqdm(num_cols):\n",
    "#         train[col] = train[col].astype(np.float16)\n",
    "#         test[col] = test[col].astype(np.float16)\n",
    "    \n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    print('all cols: ',len(features))\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40,\n",
    "        'verbose': -1,\n",
    "        }\n",
    "    \n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(X_test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "            \n",
    "        test = X_test.reset_index(drop=True)\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "\n",
    "#         for cc in tqdm(cat_features, disable=True):\n",
    "#             le = LabelEncoder()\n",
    "#             le.fit(list(set(x_val[cc]) | set(x_train[cc])))\n",
    "#             x_train[cc] = le.transform(x_train[[cc]])\n",
    "#             x_val[cc] = le.transform(x_val[[cc]])\n",
    "#             test[cc] = le.transform(test[[cc]])\n",
    "\n",
    "#             lr = LinearRegression(n_jobs=-1)\n",
    "#             ohe = OneHotEncoder(sparse=True)\n",
    "#             ohe.fit(np.arange(len(le.classes_)).reshape(-1, 1))\n",
    "#             x_ohe_train = ohe.transform(x_train[[cc]])\n",
    "#             x_ohe_val = ohe.transform(x_val[[cc]])\n",
    "#             x_ohe_test = ohe.transform(test[[cc]])\n",
    "\n",
    "#             lr.fit(x_ohe_train, y_train)\n",
    "\n",
    "#             x_train[cc] = cross_val_predict(lr, x_ohe_train, y_train, cv=kfold, n_jobs=-1)\n",
    "#             x_val[cc] = lr.predict(x_ohe_val)\n",
    "#             test[cc] = lr.predict(x_ohe_test)\n",
    "\n",
    "        print('X_train shape:', x_train.shape)\n",
    "        print('X_valid shape:', x_val.shape)\n",
    "        print('X_test shape:', test.shape)\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = add_cats+cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = add_cats+cat_features)\n",
    "            \n",
    "        if fold<=4:\n",
    "#         if fold>4:\n",
    "            \n",
    "            model = lgb.train(\n",
    "                params = params,\n",
    "                train_set = lgb_train,\n",
    "                num_boost_round = 10500,\n",
    "                valid_sets = [lgb_train, lgb_valid],\n",
    "                early_stopping_rounds = 100,\n",
    "                verbose_eval = 500,\n",
    "                feval = lgb_amex_metric\n",
    "                )\n",
    "\n",
    "            # Save best model\n",
    "            joblib.dump(model, f'../src/models/BinaryModels/lgbm_{CFG.mv}_dp{CFG.dpv}_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        \n",
    "        else:\n",
    "            with open(f'../src/models/BinaryModels/lgbm_{CFG.mv}_dp{CFG.dpv}_fold{fold}_seed{CFG.seed}.pkl', 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "                \n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    \n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    \n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_{CFG.mv}_dp{CFG.dpv}{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    \n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'lgb_{CFG.mv}_dp{CFG.dpv}{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric cols:  1713\n",
      "all cols:  2896\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2896 features...\n",
      "X_train shape: (367130, 2896)\n",
      "X_valid shape: (91783, 2896)\n",
      "X_test shape: (924621, 2897)\n",
      "[500]\ttraining's binary_logloss: 0.336165\ttraining's amex_metric: 0.778887\tvalid_1's binary_logloss: 0.338161\tvalid_1's amex_metric: 0.771466\n",
      "[1000]\ttraining's binary_logloss: 0.245346\ttraining's amex_metric: 0.795515\tvalid_1's binary_logloss: 0.251058\tvalid_1's amex_metric: 0.783145\n",
      "[1500]\ttraining's binary_logloss: 0.221166\ttraining's amex_metric: 0.809971\tvalid_1's binary_logloss: 0.230838\tvalid_1's amex_metric: 0.788561\n",
      "[2000]\ttraining's binary_logloss: 0.207353\ttraining's amex_metric: 0.822525\tvalid_1's binary_logloss: 0.222174\tvalid_1's amex_metric: 0.793939\n",
      "[2500]\ttraining's binary_logloss: 0.200202\ttraining's amex_metric: 0.832903\tvalid_1's binary_logloss: 0.219339\tvalid_1's amex_metric: 0.795865\n",
      "[3000]\ttraining's binary_logloss: 0.193066\ttraining's amex_metric: 0.842945\tvalid_1's binary_logloss: 0.217127\tvalid_1's amex_metric: 0.797403\n",
      "[3500]\ttraining's binary_logloss: 0.186484\ttraining's amex_metric: 0.853533\tvalid_1's binary_logloss: 0.215622\tvalid_1's amex_metric: 0.79952\n",
      "[4000]\ttraining's binary_logloss: 0.180621\ttraining's amex_metric: 0.863544\tvalid_1's binary_logloss: 0.214595\tvalid_1's amex_metric: 0.800513\n",
      "[4500]\ttraining's binary_logloss: 0.174909\ttraining's amex_metric: 0.873625\tvalid_1's binary_logloss: 0.213897\tvalid_1's amex_metric: 0.80111\n",
      "[5000]\ttraining's binary_logloss: 0.169321\ttraining's amex_metric: 0.883112\tvalid_1's binary_logloss: 0.2134\tvalid_1's amex_metric: 0.802045\n",
      "[5500]\ttraining's binary_logloss: 0.164278\ttraining's amex_metric: 0.891789\tvalid_1's binary_logloss: 0.213061\tvalid_1's amex_metric: 0.802294\n",
      "[6000]\ttraining's binary_logloss: 0.159987\ttraining's amex_metric: 0.899647\tvalid_1's binary_logloss: 0.212846\tvalid_1's amex_metric: 0.802377\n",
      "[6500]\ttraining's binary_logloss: 0.155474\ttraining's amex_metric: 0.906987\tvalid_1's binary_logloss: 0.21256\tvalid_1's amex_metric: 0.802718\n",
      "[7000]\ttraining's binary_logloss: 0.150251\ttraining's amex_metric: 0.915091\tvalid_1's binary_logloss: 0.212297\tvalid_1's amex_metric: 0.802524\n",
      "[7500]\ttraining's binary_logloss: 0.145326\ttraining's amex_metric: 0.923296\tvalid_1's binary_logloss: 0.212134\tvalid_1's amex_metric: 0.803259\n",
      "[8000]\ttraining's binary_logloss: 0.140954\ttraining's amex_metric: 0.930306\tvalid_1's binary_logloss: 0.212019\tvalid_1's amex_metric: 0.802994\n",
      "[8500]\ttraining's binary_logloss: 0.137222\ttraining's amex_metric: 0.936908\tvalid_1's binary_logloss: 0.21194\tvalid_1's amex_metric: 0.80315\n",
      "[9000]\ttraining's binary_logloss: 0.132989\ttraining's amex_metric: 0.942937\tvalid_1's binary_logloss: 0.211899\tvalid_1's amex_metric: 0.803578\n",
      "[9500]\ttraining's binary_logloss: 0.129203\ttraining's amex_metric: 0.948409\tvalid_1's binary_logloss: 0.211856\tvalid_1's amex_metric: 0.803886\n",
      "[10000]\ttraining's binary_logloss: 0.125486\ttraining's amex_metric: 0.953671\tvalid_1's binary_logloss: 0.211862\tvalid_1's amex_metric: 0.803589\n",
      "[10500]\ttraining's binary_logloss: 0.122265\ttraining's amex_metric: 0.958449\tvalid_1's binary_logloss: 0.211844\tvalid_1's amex_metric: 0.803218\n",
      "Our fold 0 CV score is 0.8032181446114549\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2896 features...\n",
      "X_train shape: (367130, 2896)\n",
      "X_valid shape: (91783, 2896)\n",
      "X_test shape: (924621, 2897)\n",
      "[500]\ttraining's binary_logloss: 0.335799\ttraining's amex_metric: 0.780269\tvalid_1's binary_logloss: 0.339278\tvalid_1's amex_metric: 0.763166\n",
      "[1000]\ttraining's binary_logloss: 0.24472\ttraining's amex_metric: 0.797004\tvalid_1's binary_logloss: 0.252655\tvalid_1's amex_metric: 0.775424\n",
      "[1500]\ttraining's binary_logloss: 0.220455\ttraining's amex_metric: 0.810948\tvalid_1's binary_logloss: 0.233113\tvalid_1's amex_metric: 0.781414\n",
      "[2000]\ttraining's binary_logloss: 0.20653\ttraining's amex_metric: 0.824471\tvalid_1's binary_logloss: 0.224652\tvalid_1's amex_metric: 0.785182\n",
      "[2500]\ttraining's binary_logloss: 0.199361\ttraining's amex_metric: 0.83473\tvalid_1's binary_logloss: 0.221971\tvalid_1's amex_metric: 0.787691\n",
      "[3000]\ttraining's binary_logloss: 0.192228\ttraining's amex_metric: 0.844642\tvalid_1's binary_logloss: 0.219992\tvalid_1's amex_metric: 0.789837\n",
      "[3500]\ttraining's binary_logloss: 0.18559\ttraining's amex_metric: 0.855662\tvalid_1's binary_logloss: 0.218683\tvalid_1's amex_metric: 0.791835\n",
      "[4000]\ttraining's binary_logloss: 0.179772\ttraining's amex_metric: 0.865201\tvalid_1's binary_logloss: 0.217881\tvalid_1's amex_metric: 0.792667\n",
      "[4500]\ttraining's binary_logloss: 0.174094\ttraining's amex_metric: 0.875247\tvalid_1's binary_logloss: 0.217319\tvalid_1's amex_metric: 0.793109\n",
      "[5000]\ttraining's binary_logloss: 0.168496\ttraining's amex_metric: 0.884339\tvalid_1's binary_logloss: 0.216848\tvalid_1's amex_metric: 0.793887\n",
      "[5500]\ttraining's binary_logloss: 0.163456\ttraining's amex_metric: 0.893183\tvalid_1's binary_logloss: 0.216549\tvalid_1's amex_metric: 0.793883\n",
      "[6000]\ttraining's binary_logloss: 0.159182\ttraining's amex_metric: 0.901069\tvalid_1's binary_logloss: 0.216405\tvalid_1's amex_metric: 0.794225\n",
      "[6500]\ttraining's binary_logloss: 0.154655\ttraining's amex_metric: 0.908106\tvalid_1's binary_logloss: 0.21616\tvalid_1's amex_metric: 0.794117\n",
      "[7000]\ttraining's binary_logloss: 0.149464\ttraining's amex_metric: 0.916416\tvalid_1's binary_logloss: 0.21593\tvalid_1's amex_metric: 0.794287\n",
      "[7500]\ttraining's binary_logloss: 0.144543\ttraining's amex_metric: 0.923915\tvalid_1's binary_logloss: 0.215768\tvalid_1's amex_metric: 0.794024\n",
      "[8000]\ttraining's binary_logloss: 0.140172\ttraining's amex_metric: 0.931033\tvalid_1's binary_logloss: 0.215632\tvalid_1's amex_metric: 0.794403\n",
      "[8500]\ttraining's binary_logloss: 0.136424\ttraining's amex_metric: 0.937518\tvalid_1's binary_logloss: 0.215602\tvalid_1's amex_metric: 0.793885\n",
      "[9000]\ttraining's binary_logloss: 0.132212\ttraining's amex_metric: 0.943402\tvalid_1's binary_logloss: 0.215592\tvalid_1's amex_metric: 0.794565\n",
      "[9500]\ttraining's binary_logloss: 0.128425\ttraining's amex_metric: 0.949222\tvalid_1's binary_logloss: 0.215527\tvalid_1's amex_metric: 0.794068\n",
      "[10000]\ttraining's binary_logloss: 0.124737\ttraining's amex_metric: 0.954493\tvalid_1's binary_logloss: 0.215505\tvalid_1's amex_metric: 0.794272\n",
      "[10500]\ttraining's binary_logloss: 0.121497\ttraining's amex_metric: 0.958807\tvalid_1's binary_logloss: 0.215523\tvalid_1's amex_metric: 0.794065\n",
      "Our fold 1 CV score is 0.7940652836425679\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2896 features...\n",
      "X_train shape: (367130, 2896)\n",
      "X_valid shape: (91783, 2896)\n",
      "X_test shape: (924621, 2897)\n",
      "[500]\ttraining's binary_logloss: 0.335805\ttraining's amex_metric: 0.779164\tvalid_1's binary_logloss: 0.3396\tvalid_1's amex_metric: 0.767442\n",
      "[1000]\ttraining's binary_logloss: 0.244708\ttraining's amex_metric: 0.796693\tvalid_1's binary_logloss: 0.253051\tvalid_1's amex_metric: 0.778817\n",
      "[1500]\ttraining's binary_logloss: 0.220515\ttraining's amex_metric: 0.811209\tvalid_1's binary_logloss: 0.233093\tvalid_1's amex_metric: 0.785787\n",
      "[2000]\ttraining's binary_logloss: 0.206686\ttraining's amex_metric: 0.823945\tvalid_1's binary_logloss: 0.224421\tvalid_1's amex_metric: 0.789462\n",
      "[2500]\ttraining's binary_logloss: 0.199596\ttraining's amex_metric: 0.834546\tvalid_1's binary_logloss: 0.221683\tvalid_1's amex_metric: 0.792636\n",
      "[3000]\ttraining's binary_logloss: 0.192491\ttraining's amex_metric: 0.844605\tvalid_1's binary_logloss: 0.21956\tvalid_1's amex_metric: 0.794493\n",
      "[3500]\ttraining's binary_logloss: 0.185862\ttraining's amex_metric: 0.854951\tvalid_1's binary_logloss: 0.218156\tvalid_1's amex_metric: 0.795353\n",
      "[4000]\ttraining's binary_logloss: 0.180002\ttraining's amex_metric: 0.864984\tvalid_1's binary_logloss: 0.217264\tvalid_1's amex_metric: 0.796634\n",
      "[4500]\ttraining's binary_logloss: 0.174305\ttraining's amex_metric: 0.874523\tvalid_1's binary_logloss: 0.216601\tvalid_1's amex_metric: 0.796906\n",
      "[5000]\ttraining's binary_logloss: 0.168699\ttraining's amex_metric: 0.884075\tvalid_1's binary_logloss: 0.216095\tvalid_1's amex_metric: 0.797577\n",
      "[5500]\ttraining's binary_logloss: 0.163657\ttraining's amex_metric: 0.892722\tvalid_1's binary_logloss: 0.215728\tvalid_1's amex_metric: 0.797653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6000]\ttraining's binary_logloss: 0.159339\ttraining's amex_metric: 0.901116\tvalid_1's binary_logloss: 0.215531\tvalid_1's amex_metric: 0.797906\n",
      "[6500]\ttraining's binary_logloss: 0.154796\ttraining's amex_metric: 0.908177\tvalid_1's binary_logloss: 0.2152\tvalid_1's amex_metric: 0.798342\n",
      "[7000]\ttraining's binary_logloss: 0.149592\ttraining's amex_metric: 0.916314\tvalid_1's binary_logloss: 0.214953\tvalid_1's amex_metric: 0.798296\n",
      "[7500]\ttraining's binary_logloss: 0.144691\ttraining's amex_metric: 0.924414\tvalid_1's binary_logloss: 0.214801\tvalid_1's amex_metric: 0.798503\n",
      "[8000]\ttraining's binary_logloss: 0.140342\ttraining's amex_metric: 0.931267\tvalid_1's binary_logloss: 0.214735\tvalid_1's amex_metric: 0.79818\n",
      "[8500]\ttraining's binary_logloss: 0.136613\ttraining's amex_metric: 0.937531\tvalid_1's binary_logloss: 0.21465\tvalid_1's amex_metric: 0.797786\n",
      "[9000]\ttraining's binary_logloss: 0.132391\ttraining's amex_metric: 0.943235\tvalid_1's binary_logloss: 0.214565\tvalid_1's amex_metric: 0.797893\n",
      "[9500]\ttraining's binary_logloss: 0.128611\ttraining's amex_metric: 0.949154\tvalid_1's binary_logloss: 0.214579\tvalid_1's amex_metric: 0.797952\n",
      "[10000]\ttraining's binary_logloss: 0.124901\ttraining's amex_metric: 0.954797\tvalid_1's binary_logloss: 0.21455\tvalid_1's amex_metric: 0.797985\n",
      "[10500]\ttraining's binary_logloss: 0.12167\ttraining's amex_metric: 0.959212\tvalid_1's binary_logloss: 0.214544\tvalid_1's amex_metric: 0.798204\n",
      "Our fold 2 CV score is 0.798204217737333\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2896 features...\n",
      "X_train shape: (367131, 2896)\n",
      "X_valid shape: (91782, 2896)\n",
      "X_test shape: (924621, 2897)\n",
      "[500]\ttraining's binary_logloss: 0.335465\ttraining's amex_metric: 0.780642\tvalid_1's binary_logloss: 0.340314\tvalid_1's amex_metric: 0.762391\n",
      "[1000]\ttraining's binary_logloss: 0.244442\ttraining's amex_metric: 0.797285\tvalid_1's binary_logloss: 0.254019\tvalid_1's amex_metric: 0.773704\n",
      "[1500]\ttraining's binary_logloss: 0.220288\ttraining's amex_metric: 0.8114\tvalid_1's binary_logloss: 0.23422\tvalid_1's amex_metric: 0.779343\n",
      "[2000]\ttraining's binary_logloss: 0.206401\ttraining's amex_metric: 0.824911\tvalid_1's binary_logloss: 0.22566\tvalid_1's amex_metric: 0.784017\n",
      "[2500]\ttraining's binary_logloss: 0.199278\ttraining's amex_metric: 0.83529\tvalid_1's binary_logloss: 0.222921\tvalid_1's amex_metric: 0.78655\n",
      "[3000]\ttraining's binary_logloss: 0.192172\ttraining's amex_metric: 0.845277\tvalid_1's binary_logloss: 0.220737\tvalid_1's amex_metric: 0.788749\n",
      "[3500]\ttraining's binary_logloss: 0.185595\ttraining's amex_metric: 0.855893\tvalid_1's binary_logloss: 0.219307\tvalid_1's amex_metric: 0.790501\n",
      "[4000]\ttraining's binary_logloss: 0.179743\ttraining's amex_metric: 0.86583\tvalid_1's binary_logloss: 0.218431\tvalid_1's amex_metric: 0.791337\n",
      "[4500]\ttraining's binary_logloss: 0.174059\ttraining's amex_metric: 0.875007\tvalid_1's binary_logloss: 0.21773\tvalid_1's amex_metric: 0.791634\n",
      "[5000]\ttraining's binary_logloss: 0.168432\ttraining's amex_metric: 0.884372\tvalid_1's binary_logloss: 0.217191\tvalid_1's amex_metric: 0.79224\n",
      "[5500]\ttraining's binary_logloss: 0.163449\ttraining's amex_metric: 0.893184\tvalid_1's binary_logloss: 0.216838\tvalid_1's amex_metric: 0.792318\n",
      "[6000]\ttraining's binary_logloss: 0.15916\ttraining's amex_metric: 0.901224\tvalid_1's binary_logloss: 0.216578\tvalid_1's amex_metric: 0.792507\n",
      "[6500]\ttraining's binary_logloss: 0.154643\ttraining's amex_metric: 0.908495\tvalid_1's binary_logloss: 0.216338\tvalid_1's amex_metric: 0.792538\n",
      "[7000]\ttraining's binary_logloss: 0.149417\ttraining's amex_metric: 0.916291\tvalid_1's binary_logloss: 0.216038\tvalid_1's amex_metric: 0.792961\n",
      "[7500]\ttraining's binary_logloss: 0.144507\ttraining's amex_metric: 0.924094\tvalid_1's binary_logloss: 0.215796\tvalid_1's amex_metric: 0.793558\n",
      "[8000]\ttraining's binary_logloss: 0.140138\ttraining's amex_metric: 0.931528\tvalid_1's binary_logloss: 0.215672\tvalid_1's amex_metric: 0.793489\n",
      "[8500]\ttraining's binary_logloss: 0.136393\ttraining's amex_metric: 0.93775\tvalid_1's binary_logloss: 0.21557\tvalid_1's amex_metric: 0.79353\n",
      "[9000]\ttraining's binary_logloss: 0.132164\ttraining's amex_metric: 0.943749\tvalid_1's binary_logloss: 0.215532\tvalid_1's amex_metric: 0.793538\n",
      "[9500]\ttraining's binary_logloss: 0.128385\ttraining's amex_metric: 0.949358\tvalid_1's binary_logloss: 0.215548\tvalid_1's amex_metric: 0.792966\n",
      "[10000]\ttraining's binary_logloss: 0.124657\ttraining's amex_metric: 0.9549\tvalid_1's binary_logloss: 0.215534\tvalid_1's amex_metric: 0.793921\n",
      "[10500]\ttraining's binary_logloss: 0.121438\ttraining's amex_metric: 0.959257\tvalid_1's binary_logloss: 0.215573\tvalid_1's amex_metric: 0.794016\n",
      "Our fold 3 CV score is 0.7940159606936719\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 2896 features...\n",
      "X_train shape: (367131, 2896)\n",
      "X_valid shape: (91782, 2896)\n",
      "X_test shape: (924621, 2897)\n",
      "[500]\ttraining's binary_logloss: 0.336067\ttraining's amex_metric: 0.778904\tvalid_1's binary_logloss: 0.339092\tvalid_1's amex_metric: 0.769169\n",
      "[1000]\ttraining's binary_logloss: 0.245157\ttraining's amex_metric: 0.796445\tvalid_1's binary_logloss: 0.252261\tvalid_1's amex_metric: 0.77755\n",
      "[1500]\ttraining's binary_logloss: 0.221018\ttraining's amex_metric: 0.810034\tvalid_1's binary_logloss: 0.232128\tvalid_1's amex_metric: 0.784069\n",
      "[2000]\ttraining's binary_logloss: 0.207185\ttraining's amex_metric: 0.823899\tvalid_1's binary_logloss: 0.223325\tvalid_1's amex_metric: 0.789293\n",
      "[2500]\ttraining's binary_logloss: 0.200025\ttraining's amex_metric: 0.834082\tvalid_1's binary_logloss: 0.220472\tvalid_1's amex_metric: 0.792429\n",
      "[3000]\ttraining's binary_logloss: 0.192861\ttraining's amex_metric: 0.844123\tvalid_1's binary_logloss: 0.218132\tvalid_1's amex_metric: 0.794804\n",
      "[3500]\ttraining's binary_logloss: 0.186272\ttraining's amex_metric: 0.85442\tvalid_1's binary_logloss: 0.216683\tvalid_1's amex_metric: 0.795449\n",
      "[4000]\ttraining's binary_logloss: 0.180417\ttraining's amex_metric: 0.864117\tvalid_1's binary_logloss: 0.215747\tvalid_1's amex_metric: 0.796556\n",
      "[4500]\ttraining's binary_logloss: 0.174717\ttraining's amex_metric: 0.873602\tvalid_1's binary_logloss: 0.215063\tvalid_1's amex_metric: 0.797125\n",
      "[5000]\ttraining's binary_logloss: 0.169102\ttraining's amex_metric: 0.883147\tvalid_1's binary_logloss: 0.214491\tvalid_1's amex_metric: 0.798369\n",
      "[5500]\ttraining's binary_logloss: 0.164068\ttraining's amex_metric: 0.891747\tvalid_1's binary_logloss: 0.214128\tvalid_1's amex_metric: 0.798401\n",
      "[6000]\ttraining's binary_logloss: 0.159766\ttraining's amex_metric: 0.899523\tvalid_1's binary_logloss: 0.213953\tvalid_1's amex_metric: 0.798551\n",
      "[6500]\ttraining's binary_logloss: 0.155246\ttraining's amex_metric: 0.906758\tvalid_1's binary_logloss: 0.213718\tvalid_1's amex_metric: 0.798902\n",
      "[7000]\ttraining's binary_logloss: 0.150056\ttraining's amex_metric: 0.914849\tvalid_1's binary_logloss: 0.213364\tvalid_1's amex_metric: 0.79878\n",
      "[7500]\ttraining's binary_logloss: 0.145122\ttraining's amex_metric: 0.923206\tvalid_1's binary_logloss: 0.213257\tvalid_1's amex_metric: 0.798796\n",
      "[8000]\ttraining's binary_logloss: 0.140723\ttraining's amex_metric: 0.930679\tvalid_1's binary_logloss: 0.213182\tvalid_1's amex_metric: 0.798706\n",
      "[8500]\ttraining's binary_logloss: 0.136963\ttraining's amex_metric: 0.936926\tvalid_1's binary_logloss: 0.213047\tvalid_1's amex_metric: 0.79932\n",
      "[9000]\ttraining's binary_logloss: 0.132725\ttraining's amex_metric: 0.942965\tvalid_1's binary_logloss: 0.212944\tvalid_1's amex_metric: 0.799301\n",
      "[9500]\ttraining's binary_logloss: 0.12894\ttraining's amex_metric: 0.948489\tvalid_1's binary_logloss: 0.212868\tvalid_1's amex_metric: 0.799594\n",
      "[10000]\ttraining's binary_logloss: 0.12523\ttraining's amex_metric: 0.953854\tvalid_1's binary_logloss: 0.212836\tvalid_1's amex_metric: 0.798761\n",
      "[10500]\ttraining's binary_logloss: 0.121997\ttraining's amex_metric: 0.958514\tvalid_1's binary_logloss: 0.212808\tvalid_1's amex_metric: 0.79873\n",
      "Our fold 4 CV score is 0.7987298299969645\n",
      "Our out of folds CV score is 0.7975376272588612\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(df_train, df_test, cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 75.3M/75.3M [00:07<00:00, 10.5MB/s]\n",
      "Successfully submitted to American Express - Default Prediction"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c amex-default-prediction -f lgb_v2_dpv25fold_seed42.csv -m \"add \\\n",
    "cats 2896 feats v1 CV 79753\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
